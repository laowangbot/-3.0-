#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¹¶å‘ç›‘å¬ç³»ç»Ÿä¼˜åŒ–è„šæœ¬
æå‡ç³»ç»Ÿåœ¨å¤æ‚åœºæ™¯ä¸‹çš„å¤„ç†èƒ½åŠ›
"""

import asyncio
import logging
import sys
import os
from datetime import datetime, timedelta
from typing import Dict, List, Set, Optional
import json

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class ConcurrentMonitoringOptimizer:
    """å¹¶å‘ç›‘å¬ç³»ç»Ÿä¼˜åŒ–å™¨"""
    
    def __init__(self):
        self.optimized_config = {}
        self.performance_metrics = {}
        
    def optimize_config(self):
        """ä¼˜åŒ–ç³»ç»Ÿé…ç½®"""
        print("ğŸ”§ ä¼˜åŒ–å¹¶å‘ç›‘å¬ç³»ç»Ÿé…ç½®")
        print("=" * 50)
        
        # åŸºç¡€é…ç½®ä¼˜åŒ–
        self.optimized_config = {
            # å¹¶å‘å¤„ç†ä¼˜åŒ–
            "max_concurrent_tasks": 50,  # ä»10å¢åŠ åˆ°50
            "max_user_concurrent_tasks": 100,  # ä»20å¢åŠ åˆ°100
            "batch_size": 10,  # ä»5å¢åŠ åˆ°10
            "check_interval": 3,  # ä»5ç§’å‡å°‘åˆ°3ç§’
            
            # æ¶ˆæ¯å¤„ç†ä¼˜åŒ–
            "message_delay": 0.02,  # ä»0.05å‡å°‘åˆ°0.02
            "media_group_delay": 0.5,  # ä»0.3å¢åŠ åˆ°0.5
            "max_messages_per_check": 200,  # ä»100å¢åŠ åˆ°200
            
            # APIé™åˆ¶å¤„ç†
            "api_rate_limit": 30,  # æ¯åˆ†é’Ÿæœ€å¤š30æ¬¡APIè°ƒç”¨
            "api_retry_attempts": 5,  # APIé‡è¯•æ¬¡æ•°
            "api_retry_delay": 2,  # APIé‡è¯•å»¶è¿Ÿï¼ˆç§’ï¼‰
            "api_backoff_factor": 2,  # æŒ‡æ•°é€€é¿å› å­
            
            # å†…å­˜ç®¡ç†
            "max_processed_messages": 10000,  # æœ€å¤§å­˜å‚¨å·²å¤„ç†æ¶ˆæ¯æ•°
            "message_cleanup_interval": 3600,  # æ¶ˆæ¯æ¸…ç†é—´éš”ï¼ˆç§’ï¼‰
            "memory_usage_threshold": 0.8,  # å†…å­˜ä½¿ç”¨é˜ˆå€¼
            
            # é”™è¯¯å¤„ç†
            "max_consecutive_errors": 5,  # æœ€å¤§è¿ç»­é”™è¯¯æ•°
            "error_recovery_delay": 30,  # é”™è¯¯æ¢å¤å»¶è¿Ÿï¼ˆç§’ï¼‰
            "circuit_breaker_threshold": 10,  # ç†”æ–­å™¨é˜ˆå€¼
            
            # æ€§èƒ½ç›‘æ§
            "metrics_collection_interval": 60,  # æŒ‡æ ‡æ”¶é›†é—´éš”ï¼ˆç§’ï¼‰
            "performance_alert_threshold": 0.9,  # æ€§èƒ½å‘Šè­¦é˜ˆå€¼
            "slow_task_threshold": 10,  # æ…¢ä»»åŠ¡é˜ˆå€¼ï¼ˆç§’ï¼‰
        }
        
        print("âœ… é…ç½®ä¼˜åŒ–å®Œæˆ")
        return self.optimized_config
    
    def create_enhanced_monitoring_engine(self):
        """åˆ›å»ºå¢å¼ºç‰ˆç›‘å¬å¼•æ“"""
        print("\nğŸš€ åˆ›å»ºå¢å¼ºç‰ˆç›‘å¬å¼•æ“")
        print("=" * 50)
        
        enhanced_code = '''
class EnhancedMonitoringEngine(MonitoringEngine):
    """å¢å¼ºç‰ˆç›‘å¬å¼•æ“ - æ”¯æŒé«˜å¹¶å‘å’Œå¤§è§„æ¨¡ç›‘å¬"""
    
    def __init__(self, client: Client, config: Optional[Dict[str, Any]] = None):
        super().__init__(client, config)
        
        # å¢å¼ºé…ç½®
        self.max_concurrent_tasks = config.get('max_concurrent_tasks', 50)
        self.batch_size = config.get('batch_size', 10)
        self.check_interval = config.get('check_interval', 3)
        self.api_rate_limit = config.get('api_rate_limit', 30)
        
        # æ€§èƒ½ç›‘æ§
        self.performance_metrics = {
            'total_messages_processed': 0,
            'successful_transfers': 0,
            'failed_transfers': 0,
            'api_calls_made': 0,
            'average_processing_time': 0,
            'concurrent_tasks_running': 0
        }
        
        # APIé™åˆ¶ç®¡ç†
        self.api_call_times = []
        self.last_api_call = datetime.now()
        
        # æ¶ˆæ¯å»é‡ä¼˜åŒ–
        self.processed_messages = {}  # channel_id -> {message_id: timestamp}
        self.message_cleanup_task = None
        
        # é”™è¯¯å¤„ç†
        self.consecutive_errors = 0
        self.circuit_breaker_active = False
        self.circuit_breaker_reset_time = None
        
        logger.info("å¢å¼ºç‰ˆç›‘å¬å¼•æ“åˆå§‹åŒ–å®Œæˆ")
    
    async def start_enhanced_monitoring(self):
        """å¯åŠ¨å¢å¼ºç‰ˆç›‘å¬"""
        await self.start_monitoring()
        
        # å¯åŠ¨æ¶ˆæ¯æ¸…ç†ä»»åŠ¡
        self.message_cleanup_task = asyncio.create_task(self._cleanup_processed_messages())
        
        # å¯åŠ¨æ€§èƒ½ç›‘æ§
        asyncio.create_task(self._monitor_performance())
        
        logger.info("âœ… å¢å¼ºç‰ˆç›‘å¬ç³»ç»Ÿå¯åŠ¨æˆåŠŸ")
    
    async def _cleanup_processed_messages(self):
        """å®šæœŸæ¸…ç†å·²å¤„ç†æ¶ˆæ¯è®°å½•"""
        while True:
            try:
                await asyncio.sleep(self.config.get('message_cleanup_interval', 3600))
                
                current_time = datetime.now()
                cleanup_threshold = current_time - timedelta(hours=24)
                
                for channel_id, messages in self.processed_messages.items():
                    # æ¸…ç†24å°æ—¶å‰çš„è®°å½•
                    messages_to_remove = [
                        msg_id for msg_id, timestamp in messages.items()
                        if timestamp < cleanup_threshold
                    ]
                    
                    for msg_id in messages_to_remove:
                        del messages[msg_id]
                    
                    logger.debug(f"æ¸…ç†é¢‘é“ {channel_id} çš„ {len(messages_to_remove)} æ¡è¿‡æœŸè®°å½•")
                
                logger.info("âœ… å·²å¤„ç†æ¶ˆæ¯è®°å½•æ¸…ç†å®Œæˆ")
                
            except Exception as e:
                logger.error(f"âŒ æ¸…ç†å·²å¤„ç†æ¶ˆæ¯è®°å½•å¤±è´¥: {e}")
    
    async def _monitor_performance(self):
        """ç›‘æ§ç³»ç»Ÿæ€§èƒ½"""
        while True:
            try:
                await asyncio.sleep(self.config.get('metrics_collection_interval', 60))
                
                # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
                total_processed = self.performance_metrics['total_messages_processed']
                successful = self.performance_metrics['successful_transfers']
                failed = self.performance_metrics['failed_transfers']
                
                success_rate = (successful / total_processed * 100) if total_processed > 0 else 0
                failure_rate = (failed / total_processed * 100) if total_processed > 0 else 0
                
                # æ£€æŸ¥æ€§èƒ½å‘Šè­¦
                if success_rate < self.config.get('performance_alert_threshold', 0.9) * 100:
                    logger.warning(f"âš ï¸ æ€§èƒ½å‘Šè­¦: æˆåŠŸç‡ {success_rate:.2f}% ä½äºé˜ˆå€¼")
                
                # è®°å½•æ€§èƒ½æŒ‡æ ‡
                logger.info(f"ğŸ“Š æ€§èƒ½æŒ‡æ ‡: æˆåŠŸç‡ {success_rate:.2f}%, å¤±è´¥ç‡ {failure_rate:.2f}%")
                
            except Exception as e:
                logger.error(f"âŒ æ€§èƒ½ç›‘æ§å¤±è´¥: {e}")
    
    async def _check_api_rate_limit(self):
        """æ£€æŸ¥APIè°ƒç”¨é¢‘ç‡é™åˆ¶"""
        current_time = datetime.now()
        
        # æ¸…ç†1åˆ†é’Ÿå‰çš„APIè°ƒç”¨è®°å½•
        self.api_call_times = [
            call_time for call_time in self.api_call_times
            if current_time - call_time < timedelta(minutes=1)
        ]
        
        # æ£€æŸ¥æ˜¯å¦è¶…è¿‡é™åˆ¶
        if len(self.api_call_times) >= self.api_rate_limit:
            # è®¡ç®—éœ€è¦ç­‰å¾…çš„æ—¶é—´
            oldest_call = min(self.api_call_times)
            wait_time = 60 - (current_time - oldest_call).total_seconds()
            
            if wait_time > 0:
                logger.warning(f"âš ï¸ APIè°ƒç”¨é¢‘ç‡é™åˆ¶ï¼Œç­‰å¾… {wait_time:.2f} ç§’")
                await asyncio.sleep(wait_time)
        
        # è®°å½•å½“å‰APIè°ƒç”¨
        self.api_call_times.append(current_time)
        self.performance_metrics['api_calls_made'] += 1
    
    async def _handle_api_error(self, error: Exception):
        """å¤„ç†APIé”™è¯¯"""
        self.consecutive_errors += 1
        
        if self.consecutive_errors >= self.config.get('max_consecutive_errors', 5):
            # è§¦å‘ç†”æ–­å™¨
            self.circuit_breaker_active = True
            self.circuit_breaker_reset_time = datetime.now() + timedelta(
                seconds=self.config.get('error_recovery_delay', 30)
            )
            logger.error(f"ğŸš¨ ç†”æ–­å™¨è§¦å‘: è¿ç»­é”™è¯¯ {self.consecutive_errors} æ¬¡")
        
        # æŒ‡æ•°é€€é¿é‡è¯•
        retry_delay = self.config.get('api_retry_delay', 2) * (
            self.config.get('api_backoff_factor', 2) ** self.consecutive_errors
        )
        
        logger.warning(f"âš ï¸ APIé”™è¯¯: {error}, {retry_delay} ç§’åé‡è¯•")
        await asyncio.sleep(retry_delay)
    
    async def _reset_circuit_breaker(self):
        """é‡ç½®ç†”æ–­å™¨"""
        if (self.circuit_breaker_active and 
            self.circuit_breaker_reset_time and 
            datetime.now() >= self.circuit_breaker_reset_time):
            
            self.circuit_breaker_active = False
            self.consecutive_errors = 0
            logger.info("âœ… ç†”æ–­å™¨é‡ç½®ï¼Œæ¢å¤æ­£å¸¸è¿è¡Œ")
    
    async def enhanced_poll_messages(self):
        """å¢å¼ºç‰ˆåˆ†æ‰¹è½®æ¢æ£€æŸ¥"""
        logger.info("ğŸ”„ å¯åŠ¨å¢å¼ºç‰ˆåˆ†æ‰¹è½®æ¢æ£€æŸ¥")
        
        last_message_id = {}
        current_batch = 0
        
        while True:
            try:
                # æ£€æŸ¥ç†”æ–­å™¨çŠ¶æ€
                await self._reset_circuit_breaker()
                
                if self.circuit_breaker_active:
                    logger.warning("âš ï¸ ç†”æ–­å™¨æ¿€æ´»ï¼Œè·³è¿‡æœ¬æ¬¡æ£€æŸ¥")
                    await asyncio.sleep(10)
                    continue
                
                # æ”¶é›†æ‰€æœ‰éœ€è¦æ£€æŸ¥çš„é¢‘é“
                all_channels = []
                for task_id, task in self.active_tasks.items():
                    if not task.is_running:
                        continue
                    
                    for source_channel in task.source_channels:
                        all_channels.append((task, source_channel))
                
                if not all_channels:
                    await asyncio.sleep(self.check_interval)
                    continue
                
                # åˆ†æ‰¹å¤„ç†
                total_batches = (len(all_channels) + self.batch_size - 1) // self.batch_size
                start_idx = current_batch * self.batch_size
                end_idx = min(start_idx + self.batch_size, len(all_channels))
                current_batch_channels = all_channels[start_idx:end_idx]
                
                logger.info(f"ğŸ” æ£€æŸ¥æ‰¹æ¬¡ {current_batch + 1}/{total_batches} ({len(current_batch_channels)} ä¸ªé¢‘é“)")
                
                # å¹¶å‘æ£€æŸ¥å½“å‰æ‰¹æ¬¡çš„æ‰€æœ‰é¢‘é“
                check_tasks = []
                for task, source_channel in current_batch_channels:
                    check_tasks.append(self._check_single_channel_enhanced(task, source_channel, last_message_id))
                
                if check_tasks:
                    await asyncio.gather(*check_tasks, return_exceptions=True)
                
                # ç§»åŠ¨åˆ°ä¸‹ä¸€æ‰¹æ¬¡
                current_batch = (current_batch + 1) % total_batches
                
                # ç­‰å¾…æ£€æŸ¥é—´éš”
                await asyncio.sleep(self.check_interval)
                
            except Exception as e:
                logger.error(f"âŒ å¢å¼ºç‰ˆåˆ†æ‰¹æ£€æŸ¥å¤±è´¥: {e}")
                await self._handle_api_error(e)
    
    async def _check_single_channel_enhanced(self, task, source_channel, last_message_id):
        """å¢å¼ºç‰ˆå•é¢‘é“æ£€æŸ¥"""
        try:
            # æ£€æŸ¥APIè°ƒç”¨é¢‘ç‡
            await self._check_api_rate_limit()
            
            channel_id = source_channel['channel_id']
            channel_name = source_channel.get('channel_name', 'Unknown')
            
            # è·å–é¢‘é“æœ€æ–°æ¶ˆæ¯
            messages = []
            async for message in self.client.get_chat_history(
                chat_id=channel_id, 
                limit=self.config.get('max_messages_per_check', 200)
            ):
                messages.append(message)
            
            if not messages:
                return
            
            # æ£€æŸ¥æ–°æ¶ˆæ¯
            new_messages = []
            last_id = last_message_id.get(channel_id, 0)
            
            for message in messages:
                if message.id > last_id:
                    new_messages.append(message)
            
            if new_messages:
                logger.info(f"ğŸ”” [å¢å¼ºç‰ˆ] æ£€æµ‹åˆ° {len(new_messages)} æ¡æ–°æ¶ˆæ¯ from {channel_name}")
                
                # æ›´æ–°æœ€åæ¶ˆæ¯ID
                last_message_id[channel_id] = max(msg.id for msg in new_messages)
                
                # å¤„ç†æ–°æ¶ˆæ¯
                for message in new_messages:
                    await self._handle_new_message_enhanced(task, message, source_channel)
            
        except Exception as e:
            logger.error(f"âŒ å¢å¼ºç‰ˆå•é¢‘é“æ£€æŸ¥å¤±è´¥: {e}")
            await self._handle_api_error(e)
    
    async def _handle_new_message_enhanced(self, task, message, source_config):
        """å¢å¼ºç‰ˆæ–°æ¶ˆæ¯å¤„ç†"""
        try:
            # æ¶ˆæ¯å»é‡æ£€æŸ¥
            channel_id = str(message.chat.id)
            if channel_id in self.processed_messages:
                if message.id in self.processed_messages[channel_id]:
                    return  # å·²å¤„ç†è¿‡
            
            # æ·»åŠ åˆ°å·²å¤„ç†é›†åˆ
            if channel_id not in self.processed_messages:
                self.processed_messages[channel_id] = {}
            self.processed_messages[channel_id][message.id] = datetime.now()
            
            # å¤„ç†æ¶ˆæ¯
            await self._handle_new_message(task, message, source_config)
            
            # æ›´æ–°æ€§èƒ½æŒ‡æ ‡
            self.performance_metrics['total_messages_processed'] += 1
            
        except Exception as e:
            logger.error(f"âŒ å¢å¼ºç‰ˆæ¶ˆæ¯å¤„ç†å¤±è´¥: {e}")
            self.performance_metrics['failed_transfers'] += 1
'''
        
        print("âœ… å¢å¼ºç‰ˆç›‘å¬å¼•æ“ä»£ç ç”Ÿæˆå®Œæˆ")
        return enhanced_code
    
    def create_configuration_manager(self):
        """åˆ›å»ºé…ç½®ç®¡ç†å™¨"""
        print("\nâš™ï¸ åˆ›å»ºé…ç½®ç®¡ç†å™¨")
        print("=" * 50)
        
        config_manager_code = '''
class ConfigurationManager:
    """é…ç½®ç®¡ç†å™¨ - æ”¯æŒå¤æ‚ç›‘å¬å…³ç³»é…ç½®"""
    
    def __init__(self):
        self.channel_pairs = {}  # ç›‘å¬å…³ç³»é…ç½®
        self.channel_filters = {}  # é¢‘é“è¿‡æ»¤é…ç½®
        self.performance_config = {}  # æ€§èƒ½é…ç½®
        
    def add_channel_pair(self, source_channel: str, target_channel: str, 
                        filter_config: Dict = None, priority: int = 1):
        """æ·»åŠ é¢‘é“ç›‘å¬å…³ç³»"""
        pair_id = f"{source_channel}->{target_channel}"
        
        self.channel_pairs[pair_id] = {
            'source': source_channel,
            'target': target_channel,
            'filter_config': filter_config or {},
            'priority': priority,
            'enabled': True,
            'created_at': datetime.now()
        }
        
        logger.info(f"âœ… æ·»åŠ é¢‘é“ç›‘å¬å…³ç³»: {pair_id}")
    
    def add_multi_target_pair(self, source_channel: str, target_channels: List[str], 
                             filter_config: Dict = None):
        """æ·»åŠ ä¸€å¯¹å¤šç›‘å¬å…³ç³»"""
        for target_channel in target_channels:
            self.add_channel_pair(source_channel, target_channel, filter_config)
        
        logger.info(f"âœ… æ·»åŠ ä¸€å¯¹å¤šç›‘å¬å…³ç³»: {source_channel} -> {len(target_channels)} ä¸ªç›®æ ‡")
    
    def add_multi_source_pair(self, source_channels: List[str], target_channel: str, 
                             filter_config: Dict = None):
        """æ·»åŠ å¤šå¯¹ä¸€ç›‘å¬å…³ç³»"""
        for source_channel in source_channels:
            self.add_channel_pair(source_channel, target_channel, filter_config)
        
        logger.info(f"âœ… æ·»åŠ å¤šå¯¹ä¸€ç›‘å¬å…³ç³»: {len(source_channels)} ä¸ªæº -> {target_channel}")
    
    def validate_configuration(self):
        """éªŒè¯é…ç½®æœ‰æ•ˆæ€§"""
        errors = []
        
        # æ£€æŸ¥é‡å¤é…ç½®
        source_target_pairs = set()
        for pair_id, config in self.channel_pairs.items():
            pair = (config['source'], config['target'])
            if pair in source_target_pairs:
                errors.append(f"é‡å¤é…ç½®: {pair_id}")
            source_target_pairs.add(pair)
        
        # æ£€æŸ¥é…ç½®å®Œæ•´æ€§
        for pair_id, config in self.channel_pairs.items():
            if not config.get('source') or not config.get('target'):
                errors.append(f"é…ç½®ä¸å®Œæ•´: {pair_id}")
        
        if errors:
            logger.error(f"âŒ é…ç½®éªŒè¯å¤±è´¥: {errors}")
            return False
        
        logger.info("âœ… é…ç½®éªŒè¯é€šè¿‡")
        return True
    
    def get_optimized_batch_config(self, total_channels: int):
        """è·å–ä¼˜åŒ–çš„åˆ†æ‰¹é…ç½®"""
        if total_channels <= 10:
            return {'batch_size': 5, 'check_interval': 2}
        elif total_channels <= 30:
            return {'batch_size': 10, 'check_interval': 3}
        elif total_channels <= 50:
            return {'batch_size': 15, 'check_interval': 4}
        else:
            return {'batch_size': 20, 'check_interval': 5}
    
    def export_configuration(self, file_path: str):
        """å¯¼å‡ºé…ç½®åˆ°æ–‡ä»¶"""
        config_data = {
            'channel_pairs': self.channel_pairs,
            'channel_filters': self.channel_filters,
            'performance_config': self.performance_config,
            'exported_at': datetime.now().isoformat()
        }
        
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(config_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"âœ… é…ç½®å·²å¯¼å‡ºåˆ°: {file_path}")
    
    def import_configuration(self, file_path: str):
        """ä»æ–‡ä»¶å¯¼å…¥é…ç½®"""
        with open(file_path, 'r', encoding='utf-8') as f:
            config_data = json.load(f)
        
        self.channel_pairs = config_data.get('channel_pairs', {})
        self.channel_filters = config_data.get('channel_filters', {})
        self.performance_config = config_data.get('performance_config', {})
        
        logger.info(f"âœ… é…ç½®å·²ä»æ–‡ä»¶å¯¼å…¥: {file_path}")
'''
        
        print("âœ… é…ç½®ç®¡ç†å™¨ä»£ç ç”Ÿæˆå®Œæˆ")
        return config_manager_code
    
    def create_performance_monitor(self):
        """åˆ›å»ºæ€§èƒ½ç›‘æ§å™¨"""
        print("\nğŸ“Š åˆ›å»ºæ€§èƒ½ç›‘æ§å™¨")
        print("=" * 50)
        
        monitor_code = '''
class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§å™¨ - å®æ—¶ç›‘æ§ç³»ç»Ÿæ€§èƒ½"""
    
    def __init__(self):
        self.metrics = {
            'start_time': datetime.now(),
            'total_messages': 0,
            'successful_transfers': 0,
            'failed_transfers': 0,
            'api_calls': 0,
            'average_response_time': 0,
            'peak_concurrent_tasks': 0,
            'current_concurrent_tasks': 0,
            'memory_usage': 0,
            'error_rate': 0
        }
        
        self.alert_thresholds = {
            'error_rate': 0.1,  # 10%é”™è¯¯ç‡å‘Šè­¦
            'response_time': 5.0,  # 5ç§’å“åº”æ—¶é—´å‘Šè­¦
            'memory_usage': 0.8,  # 80%å†…å­˜ä½¿ç”¨å‘Šè­¦
            'concurrent_tasks': 0.9  # 90%å¹¶å‘ä»»åŠ¡å‘Šè­¦
        }
    
    def update_metrics(self, **kwargs):
        """æ›´æ–°æ€§èƒ½æŒ‡æ ‡"""
        for key, value in kwargs.items():
            if key in self.metrics:
                self.metrics[key] = value
        
        # è®¡ç®—æ´¾ç”ŸæŒ‡æ ‡
        self.metrics['error_rate'] = (
            self.metrics['failed_transfers'] / 
            max(self.metrics['total_messages'], 1)
        )
        
        # æ£€æŸ¥å‘Šè­¦
        self._check_alerts()
    
    def _check_alerts(self):
        """æ£€æŸ¥æ€§èƒ½å‘Šè­¦"""
        alerts = []
        
        if self.metrics['error_rate'] > self.alert_thresholds['error_rate']:
            alerts.append(f"é”™è¯¯ç‡è¿‡é«˜: {self.metrics['error_rate']:.2%}")
        
        if self.metrics['average_response_time'] > self.alert_thresholds['response_time']:
            alerts.append(f"å“åº”æ—¶é—´è¿‡é•¿: {self.metrics['average_response_time']:.2f}ç§’")
        
        if self.metrics['memory_usage'] > self.alert_thresholds['memory_usage']:
            alerts.append(f"å†…å­˜ä½¿ç”¨è¿‡é«˜: {self.metrics['memory_usage']:.2%}")
        
        if alerts:
            logger.warning(f"âš ï¸ æ€§èƒ½å‘Šè­¦: {'; '.join(alerts)}")
    
    def get_performance_report(self):
        """è·å–æ€§èƒ½æŠ¥å‘Š"""
        uptime = datetime.now() - self.metrics['start_time']
        
        report = {
            'uptime': str(uptime),
            'total_messages': self.metrics['total_messages'],
            'success_rate': (
                self.metrics['successful_transfers'] / 
                max(self.metrics['total_messages'], 1) * 100
            ),
            'error_rate': self.metrics['error_rate'] * 100,
            'average_response_time': self.metrics['average_response_time'],
            'peak_concurrent_tasks': self.metrics['peak_concurrent_tasks'],
            'current_concurrent_tasks': self.metrics['current_concurrent_tasks'],
            'memory_usage': self.metrics['memory_usage'] * 100,
            'api_calls_per_minute': (
                self.metrics['api_calls'] / 
                max(uptime.total_seconds() / 60, 1)
            )
        }
        
        return report
    
    def log_performance_summary(self):
        """è®°å½•æ€§èƒ½æ‘˜è¦"""
        report = self.get_performance_report()
        
        logger.info("ğŸ“Š æ€§èƒ½æ‘˜è¦:")
        logger.info(f"  è¿è¡Œæ—¶é—´: {report['uptime']}")
        logger.info(f"  æ€»æ¶ˆæ¯æ•°: {report['total_messages']}")
        logger.info(f"  æˆåŠŸç‡: {report['success_rate']:.2f}%")
        logger.info(f"  é”™è¯¯ç‡: {report['error_rate']:.2f}%")
        logger.info(f"  å¹³å‡å“åº”æ—¶é—´: {report['average_response_time']:.2f}ç§’")
        logger.info(f"  å½“å‰å¹¶å‘ä»»åŠ¡: {report['current_concurrent_tasks']}")
        logger.info(f"  å†…å­˜ä½¿ç”¨: {report['memory_usage']:.2f}%")
        logger.info(f"  APIè°ƒç”¨é¢‘ç‡: {report['api_calls_per_minute']:.2f}æ¬¡/åˆ†é’Ÿ")
'''
        
        print("âœ… æ€§èƒ½ç›‘æ§å™¨ä»£ç ç”Ÿæˆå®Œæˆ")
        return monitor_code
    
    def generate_optimization_report(self):
        """ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š"""
        print("\nğŸ“‹ ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š")
        print("=" * 50)
        
        report = {
            'optimization_time': datetime.now().isoformat(),
            'original_config': {
                'max_concurrent_tasks': 10,
                'batch_size': 5,
                'check_interval': 5,
                'max_messages_per_check': 100
            },
            'optimized_config': self.optimized_config,
            'performance_improvements': {
                'concurrent_capacity': '5x increase (10 -> 50)',
                'check_frequency': '1.67x increase (5s -> 3s)',
                'batch_processing': '2x increase (5 -> 10)',
                'message_throughput': '2x increase (100 -> 200)',
                'api_rate_limiting': 'Added',
                'error_recovery': 'Added',
                'performance_monitoring': 'Added'
            },
            'recommended_settings': {
                'for_30_channels': {
                    'max_concurrent_tasks': 50,
                    'batch_size': 10,
                    'check_interval': 3,
                    'max_messages_per_check': 200
                },
                'for_50_channels': {
                    'max_concurrent_tasks': 75,
                    'batch_size': 15,
                    'check_interval': 4,
                    'max_messages_per_check': 300
                },
                'for_100_channels': {
                    'max_concurrent_tasks': 100,
                    'batch_size': 20,
                    'check_interval': 5,
                    'max_messages_per_check': 500
                }
            }
        }
        
        print("âœ… ä¼˜åŒ–æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
        return report

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¹¶å‘ç›‘å¬ç³»ç»Ÿä¼˜åŒ–å·¥å…·")
    print("=" * 50)
    print("æ­¤å·¥å…·å°†ä¼˜åŒ–ç³»ç»Ÿä»¥æ”¯æŒ30ä¸ªæºé¢‘é“å’Œ20ä¸ªç›®æ ‡é¢‘é“çš„å¤æ‚ç›‘å¬åœºæ™¯")
    print()
    
    optimizer = ConcurrentMonitoringOptimizer()
    
    # 1. ä¼˜åŒ–é…ç½®
    config = optimizer.optimize_config()
    print(f"âœ… ä¼˜åŒ–é…ç½®: {config}")
    
    # 2. ç”Ÿæˆå¢å¼ºç‰ˆç›‘å¬å¼•æ“
    enhanced_engine = optimizer.create_enhanced_monitoring_engine()
    
    # 3. ç”Ÿæˆé…ç½®ç®¡ç†å™¨
    config_manager = optimizer.create_configuration_manager()
    
    # 4. ç”Ÿæˆæ€§èƒ½ç›‘æ§å™¨
    performance_monitor = optimizer.create_performance_monitor()
    
    # 5. ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š
    report = optimizer.generate_optimization_report()
    
    print("\nğŸ¯ ä¼˜åŒ–æ€»ç»“:")
    print("1. âœ… å¹¶å‘å¤„ç†èƒ½åŠ›æå‡5å€ (10 -> 50)")
    print("2. âœ… æ£€æŸ¥é¢‘ç‡æå‡1.67å€ (5ç§’ -> 3ç§’)")
    print("3. âœ… æ‰¹å¤„ç†èƒ½åŠ›æå‡2å€ (5 -> 10)")
    print("4. âœ… æ¶ˆæ¯ååé‡æå‡2å€ (100 -> 200)")
    print("5. âœ… æ·»åŠ APIé™åˆ¶å¤„ç†")
    print("6. âœ… æ·»åŠ é”™è¯¯æ¢å¤æœºåˆ¶")
    print("7. âœ… æ·»åŠ æ€§èƒ½ç›‘æ§")
    print("8. âœ… æ·»åŠ é…ç½®ç®¡ç†")
    
    print("\nğŸ’¡ ä½¿ç”¨å»ºè®®:")
    print("- æ ¹æ®æ‚¨çš„30ä¸ªæºé¢‘é“åœºæ™¯ï¼Œå»ºè®®ä½¿ç”¨æ¨èçš„é…ç½®")
    print("- ç›‘æ§ç³»ç»Ÿæ€§èƒ½ï¼Œæ ¹æ®å®é™…æƒ…å†µè°ƒæ•´å‚æ•°")
    print("- å®šæœŸæ¸…ç†å·²å¤„ç†æ¶ˆæ¯è®°å½•ï¼Œé¿å…å†…å­˜æ³„æ¼")
    print("- è®¾ç½®æ€§èƒ½å‘Šè­¦ï¼ŒåŠæ—¶å‘ç°å’Œå¤„ç†é—®é¢˜")

if __name__ == "__main__":
    main()


