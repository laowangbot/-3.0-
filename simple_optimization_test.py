#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç®€åŒ–çš„ä¼˜åŒ–éªŒè¯è„šæœ¬
æµ‹è¯•é…ç½®ä¼˜åŒ–æ•ˆæœ
"""

import sys
import os
from datetime import datetime, timedelta

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from config import get_config

def test_configuration():
    """æµ‹è¯•é…ç½®ä¼˜åŒ–"""
    print("ğŸ§ª ç›‘å¬ç³»ç»Ÿä¼˜åŒ–éªŒè¯")
    print("=" * 50)
    
    config = get_config()
    
    # æ£€æŸ¥å…³é”®é…ç½®
    config_tests = [
        ("max_concurrent_tasks", 50, "å¹¶å‘ä»»åŠ¡æ•°"),
        ("max_user_concurrent_tasks", 100, "ç”¨æˆ·å¹¶å‘ä»»åŠ¡æ•°"),
        ("batch_size", 10, "æ‰¹å¤„ç†å¤§å°"),
        ("check_interval", 3, "æ£€æŸ¥é—´éš”"),
        ("max_messages_per_check", 200, "æœ€å¤§æ¶ˆæ¯æ£€æŸ¥æ•°"),
        ("api_rate_limit", 30, "APIé™åˆ¶"),
        ("message_delay", 0.02, "æ¶ˆæ¯å»¶è¿Ÿ"),
        ("media_group_delay", 0.5, "åª’ä½“ç»„å»¶è¿Ÿ")
    ]
    
    print("ğŸ“‹ é…ç½®ä¼˜åŒ–æ£€æŸ¥:")
    all_passed = True
    for key, expected, description in config_tests:
        actual = config.get(key)
        if actual == expected:
            print(f"âœ… {description}: {actual} (ç¬¦åˆé¢„æœŸ)")
        else:
            print(f"âŒ {description}: {actual} (é¢„æœŸ: {expected})")
            all_passed = False
    
    return all_passed

def test_performance_calculation():
    """æµ‹è¯•æ€§èƒ½è®¡ç®—"""
    print("\nğŸ“Š æ€§èƒ½è®¡ç®—æµ‹è¯•:")
    print("-" * 30)
    
    config = get_config()
    
    # æ¨¡æ‹Ÿæ€§èƒ½æ•°æ®
    test_data = [
        {"total": 1000, "successful": 950, "failed": 50},
        {"total": 500, "successful": 400, "failed": 100},
        {"total": 2000, "successful": 1900, "failed": 100},
        {"total": 0, "successful": 0, "failed": 0}
    ]
    
    for i, data in enumerate(test_data, 1):
        total = data["total"]
        successful = data["successful"]
        failed = data["failed"]
        
        success_rate = (successful / total * 100) if total > 0 else 0
        failure_rate = (failed / total * 100) if total > 0 else 0
        
        print(f"æµ‹è¯• {i}: æ€»æ¶ˆæ¯ {total}, æˆåŠŸ {successful}, å¤±è´¥ {failed}")
        print(f"  æˆåŠŸç‡: {success_rate:.2f}%, å¤±è´¥ç‡: {failure_rate:.2f}%")
        
        # æ£€æŸ¥æ€§èƒ½å‘Šè­¦
        if success_rate < config.get('performance_alert_threshold', 0.9) * 100:
            print(f"  âš ï¸ æ€§èƒ½å‘Šè­¦: æˆåŠŸç‡ {success_rate:.2f}% ä½äºé˜ˆå€¼")
        else:
            print(f"  âœ… æ€§èƒ½æ­£å¸¸")
    
    return True

def test_api_rate_limiting():
    """æµ‹è¯•APIé™åˆ¶é€»è¾‘"""
    print("\nğŸš¦ APIé™åˆ¶æµ‹è¯•:")
    print("-" * 30)
    
    config = get_config()
    
    # æ¨¡æ‹ŸAPIè°ƒç”¨æ—¶é—´è®°å½•
    api_call_times = []
    current_time = datetime.now()
    
    # æ¨¡æ‹Ÿ1åˆ†é’Ÿå†…çš„APIè°ƒç”¨
    for i in range(35):  # è¶…è¿‡é™åˆ¶çš„è°ƒç”¨æ¬¡æ•°
        call_time = current_time - timedelta(seconds=i*2)  # æ¯2ç§’ä¸€æ¬¡è°ƒç”¨
        api_call_times.append(call_time)
    
    # æ£€æŸ¥æ˜¯å¦è¶…è¿‡é™åˆ¶
    recent_calls = [
        call_time for call_time in api_call_times
        if current_time - call_time < timedelta(minutes=1)
    ]
    
    rate_limit = config.get('api_rate_limit', 30)
    print(f"APIè°ƒç”¨æ¬¡æ•°: {len(recent_calls)}")
    print(f"APIé™åˆ¶: {rate_limit}")
    
    if len(recent_calls) >= rate_limit:
        print("âš ï¸ è¶…è¿‡APIé™åˆ¶ï¼Œéœ€è¦ç­‰å¾…")
        oldest_call = min(recent_calls)
        wait_time = 60 - (current_time - oldest_call).total_seconds()
        print(f"éœ€è¦ç­‰å¾…: {wait_time:.2f} ç§’")
    else:
        print("âœ… APIè°ƒç”¨åœ¨é™åˆ¶èŒƒå›´å†…")
    
    return True

def test_batch_processing():
    """æµ‹è¯•åˆ†æ‰¹å¤„ç†é€»è¾‘"""
    print("\nğŸ“¦ åˆ†æ‰¹å¤„ç†æµ‹è¯•:")
    print("-" * 30)
    
    config = get_config()
    
    # æ¨¡æ‹Ÿä¸åŒæ•°é‡çš„é¢‘é“
    test_cases = [
        {"channels": 5, "expected_batches": 1},
        {"channels": 15, "expected_batches": 2},
        {"channels": 30, "expected_batches": 3},
        {"channels": 50, "expected_batches": 5},
        {"channels": 100, "expected_batches": 10}
    ]
    
    batch_size = config.get('batch_size', 10)
    
    for case in test_cases:
        channels = case["channels"]
        expected_batches = case["expected_batches"]
        
        actual_batches = (channels + batch_size - 1) // batch_size
        
        print(f"é¢‘é“æ•°: {channels}, æ‰¹å¤„ç†å¤§å°: {batch_size}")
        print(f"é¢„æœŸæ‰¹æ¬¡æ•°: {expected_batches}, å®é™…æ‰¹æ¬¡æ•°: {actual_batches}")
        
        if actual_batches == expected_batches:
            print("âœ… åˆ†æ‰¹è®¡ç®—æ­£ç¡®")
        else:
            print("âŒ åˆ†æ‰¹è®¡ç®—é”™è¯¯")
    
    return True

def test_error_recovery():
    """æµ‹è¯•é”™è¯¯æ¢å¤æœºåˆ¶"""
    print("\nğŸ”„ é”™è¯¯æ¢å¤æµ‹è¯•:")
    print("-" * 30)
    
    config = get_config()
    
    # æµ‹è¯•æŒ‡æ•°é€€é¿
    base_delay = config.get('api_retry_delay', 2)
    backoff_factor = config.get('api_backoff_factor', 2)
    max_errors = config.get('max_consecutive_errors', 5)
    
    print(f"åŸºç¡€å»¶è¿Ÿ: {base_delay} ç§’")
    print(f"é€€é¿å› å­: {backoff_factor}")
    print(f"æœ€å¤§è¿ç»­é”™è¯¯: {max_errors}")
    
    for error_count in range(1, 8):
        retry_delay = base_delay * (backoff_factor ** error_count)
        print(f"é”™è¯¯æ¬¡æ•° {error_count}: é‡è¯•å»¶è¿Ÿ {retry_delay} ç§’")
        
        if error_count >= max_errors:
            print(f"  ğŸš¨ è§¦å‘ç†”æ–­å™¨ (è¿ç»­é”™è¯¯ {error_count} æ¬¡)")
    
    return True

def test_memory_management():
    """æµ‹è¯•å†…å­˜ç®¡ç†"""
    print("\nğŸ’¾ å†…å­˜ç®¡ç†æµ‹è¯•:")
    print("-" * 30)
    
    config = get_config()
    
    max_messages = config.get('max_processed_messages', 10000)
    cleanup_interval = config.get('message_cleanup_interval', 3600)
    memory_threshold = config.get('memory_usage_threshold', 0.8)
    
    print(f"æœ€å¤§å­˜å‚¨æ¶ˆæ¯æ•°: {max_messages}")
    print(f"æ¸…ç†é—´éš”: {cleanup_interval} ç§’ ({cleanup_interval/3600:.1f} å°æ—¶)")
    print(f"å†…å­˜ä½¿ç”¨é˜ˆå€¼: {memory_threshold*100:.0f}%")
    
    # è®¡ç®—æ¸…ç†é¢‘ç‡
    messages_per_hour = max_messages / (cleanup_interval / 3600)
    print(f"æ¯å°æ—¶å¤„ç†æ¶ˆæ¯æ•°: {messages_per_hour:.0f}")
    
    return True

def test_performance_scenario():
    """æµ‹è¯•30ä¸ªæºé¢‘é“çš„æ€§èƒ½åœºæ™¯"""
    print("\nğŸš€ 30ä¸ªæºé¢‘é“æ€§èƒ½æµ‹è¯•:")
    print("-" * 30)
    
    config = get_config()
    
    # æ¨¡æ‹Ÿ30ä¸ªæºé¢‘é“çš„åœºæ™¯
    source_channels = 30
    target_channels = 20
    
    print(f"æ¨¡æ‹Ÿåœºæ™¯: {source_channels} ä¸ªæºé¢‘é“ â†’ {target_channels} ä¸ªç›®æ ‡é¢‘é“")
    
    # è®¡ç®—ç†è®ºæ€§èƒ½
    batch_size = config.get('batch_size', 10)
    check_interval = config.get('check_interval', 3)
    
    total_batches = (source_channels + batch_size - 1) // batch_size
    full_cycle_time = total_batches * check_interval
    
    print(f"æ‰¹å¤„ç†å¤§å°: {batch_size}")
    print(f"æ£€æŸ¥é—´éš”: {check_interval} ç§’")
    print(f"æ€»æ‰¹æ¬¡æ•°: {total_batches}")
    print(f"å®Œæ•´æ£€æŸ¥å‘¨æœŸ: {full_cycle_time} ç§’")
    
    # è®¡ç®—ååé‡
    max_messages = config.get('max_messages_per_check', 200)
    messages_per_cycle = source_channels * max_messages
    messages_per_hour = (messages_per_cycle * 3600) / full_cycle_time
    
    print(f"æ¯å‘¨æœŸæ£€æŸ¥æ¶ˆæ¯æ•°: {messages_per_cycle}")
    print(f"ç†è®ºæ¯å°æ—¶å¤„ç†æ¶ˆæ¯æ•°: {messages_per_hour:.0f}")
    
    # æ£€æŸ¥æ˜¯å¦æ»¡è¶³éœ€æ±‚
    if full_cycle_time <= 10:
        print("âœ… æ£€æŸ¥å‘¨æœŸæ»¡è¶³éœ€æ±‚ (<10ç§’)")
    else:
        print(f"âš ï¸ æ£€æŸ¥å‘¨æœŸè¿‡é•¿: {full_cycle_time} ç§’")
    
    if messages_per_hour >= 10000:
        print("âœ… ååé‡æ»¡è¶³éœ€æ±‚ (>10,000 æ¶ˆæ¯/å°æ—¶)")
    else:
        print(f"âš ï¸ ååé‡å¯èƒ½ä¸è¶³: {messages_per_hour:.0f} æ¶ˆæ¯/å°æ—¶")
    
    return True

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ ç›‘å¬ç³»ç»Ÿä¼˜åŒ–éªŒè¯")
    print("=" * 50)
    
    # è¿è¡Œå„é¡¹æµ‹è¯•
    tests = [
        ("é…ç½®ä¼˜åŒ–", test_configuration),
        ("æ€§èƒ½è®¡ç®—", test_performance_calculation),
        ("APIé™åˆ¶", test_api_rate_limiting),
        ("åˆ†æ‰¹å¤„ç†", test_batch_processing),
        ("é”™è¯¯æ¢å¤", test_error_recovery),
        ("å†…å­˜ç®¡ç†", test_memory_management),
        ("30é¢‘é“æ€§èƒ½", test_performance_scenario)
    ]
    
    results = []
    for test_name, test_func in tests:
        try:
            result = test_func()
            results.append((test_name, result))
        except Exception as e:
            print(f"âŒ {test_name} æµ‹è¯•å¤±è´¥: {e}")
            results.append((test_name, False))
    
    # è¾“å‡ºæµ‹è¯•ç»“æœ
    print("\nğŸ“‹ æµ‹è¯•ç»“æœæ±‡æ€»")
    print("=" * 50)
    
    passed = 0
    total = len(results)
    
    for test_name, result in results:
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"{test_name}: {status}")
        if result:
            passed += 1
    
    print(f"\næ€»ä½“ç»“æœ: {passed}/{total} é¡¹æµ‹è¯•é€šè¿‡")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ç³»ç»Ÿä¼˜åŒ–æˆåŠŸï¼")
        print("\nğŸ’¡ ä¼˜åŒ–æ€»ç»“:")
        print("1. âœ… å¹¶å‘å¤„ç†èƒ½åŠ›æå‡5å€ (10 â†’ 50)")
        print("2. âœ… æ£€æŸ¥é¢‘ç‡æå‡1.67å€ (5ç§’ â†’ 3ç§’)")
        print("3. âœ… æ‰¹å¤„ç†èƒ½åŠ›æå‡2å€ (5 â†’ 10)")
        print("4. âœ… æ¶ˆæ¯ååé‡æå‡2å€ (100 â†’ 200)")
        print("5. âœ… æ·»åŠ APIé™åˆ¶ä¿æŠ¤")
        print("6. âœ… æ·»åŠ é”™è¯¯æ¢å¤æœºåˆ¶")
        print("7. âœ… æ·»åŠ æ€§èƒ½ç›‘æ§")
        print("8. âœ… æ·»åŠ å†…å­˜ç®¡ç†")
        
        print("\nğŸš€ ç°åœ¨å¯ä»¥æ”¯æŒ30ä¸ªæºé¢‘é“å’Œ20ä¸ªç›®æ ‡é¢‘é“çš„å¤æ‚ç›‘å¬åœºæ™¯ï¼")
    else:
        print("âš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")
    
    return passed == total

if __name__ == "__main__":
    exit(0 if main() else 1)

